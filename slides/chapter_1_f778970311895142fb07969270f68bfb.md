---
title: Insert title here
key: f778970311895142fb07969270f68bfb

---
## Q9: How do you calculate needed sample size?

```yaml
type: "TitleSlide"
key: "c8e4861b47"
```

`@lower_third`

name: Conor Dewey
title: Data Scientist, Squarespace


`@script`
In the last lesson, we addressed analyzing results of an A/B test and deciding whether or not to ship the experiment. 

Now we'll take things a step further by looking at another popular interview question: How do you calculate needed sample size?


---
## Setting the scene

```yaml
type: "FullSlide"
key: "09d1b0c7bf"
```

`@part1`
* In other words: how many samples are needed per treatment?{{1}}
   * Advised that you decide on a sample size prior to the experiment{{2}}
   * No good general guidance when it comes to sample size; context dependent{{3}}
   * In practice, the method we'll use to solve this is referred to as **power analysis**.{{4}}


`@script`
Let's provide a little context before we dive in.

If you run an experiment, how do you decide how long it should run, or in our terms: how many samples are needed per treatment?

This question is relevant because it's normally advised that you decide on a sample size before you start an experiment.

Despite what you may read in many guides to A/B testing, there's no good general guidance here â€” as is often the case, the answer is: it depends.

In practice, the approach to solving this problem we'll use is often referred to as power analysis.


---
## Moving parts

```yaml
type: "FullSlide"
key: "2a28ffdaae"
```

`@part1`
* **Sample size:** number of observations needed per treatment{{1}}
* **Effect size:** minimum size of the effect that you want to detect{{2}}
* **Significance level:** significance level at which the test will be conducted{{3}}
* **Power:** probability of detecting a specified effect size{{4}}


`@script`
At a high-level, there are four moving parts here: sample size, minimum effect size, significance level, and power.

Specify any of the three, and the fourth can be calculated. Most commonly, you'll want to calculate sample size, so you must specify the other three.

First, the minimum size of the effect that you want to detect in a test, for example: a 20% improvement. The smaller the effect you want to detect, the more observations you're going to need.

Next is the significance level at which the test will be conducted, commonly referred to as the alpha value. Once again, the lower our significance level, the more observations we'll need.

Last is power, the probability of detecting an effect. The key thing here is that if we see something interesting, we want to make sure that we have enough power to conclude with high probability that the result is actually statistically significant. If we want more power, you guessed it: we'll need a bigger sample.


---
## Calculating sample size

```yaml
type: "FullSlide"
key: "b94f8b2f28"
```

`@part1`
* `statsmodels.stats.power`{{1}}
   * `zt_ind_solve_power():` solves for any one parameter of a two sample z-test
   * `tt_ind_solve_power():` solves for any one parameter of two sample t-test
* `statsmodels.stats.proportion`{{2}}
   * `proportion_effectsize():` computes standard effect size for a test comparing two proportions


`@script`
Now that we understand all the components that go into calculating sample size, let's get our hands dirty in Python.

We'll be using a couple functions within the `statsmodels` package. Notably, the `zt_ind_solve_power` and `tt_ind_solve_power` functions which take in all but one of the aforementioned components and then calculate the remaining parameter.

However, one preliminary step must be taken. The power functions above require standardized minimum effect difference. To get this, we can use the `proportion_effectsize` function by inputting our baseline and desired minimum conversion rates.


---
## Example

```yaml
type: "FullCodeSlide"
key: "e209e0c32d"
```

`@part1`
You are working with a website and want to test for a difference in conversion rates. Before you begin the experiment, you must decide how many samples you'll need per variant. You've been instructed to use 80% power, 5% significance, and detect a minimum effect size of 5%. The current baseline conversion rate is 20% and you can assume a 50/50 split between control and treatment. 

**How many samples will you need per group?
** 
```python
from statsmodels.stats.power import  zt_ind_solve_power
import statsmodels.stats.proportion as prop

std_effect = prop.proportion_effectsize(.20, .25)
zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=.05, power=.80)
```{{1}}

```
1091.8962
```{{2}}


`@script`
Let's look at an example. Since both functions work essentially the same, we'll focus on a z-test scenario where we are looking at conversion rates of a website. Feel free to pause the video here and read through the problem in its entirety.

First we input our baseline conversion rate of 20% and then add our minimum effect size of 5% to get 25% as the second proportion. Once we have this result, we can plug it into our power function along with the commonly used significance value of 0.05 and power value of 0.80. Once we run this function, we get our desired sample of around 1091 impressions.


---
## Example

```yaml
type: "FullCodeSlide"
key: "a7eb14f7cb"
disable_transition: true
```

`@part1`
You are working with a website and want to test for a difference in conversion rates. Before you begin the experiment, you must decide how many samples you'll need per variant. You've been instructed to use **95% power**, 5% significance, and detect a minimum effect size of 5%. The current baseline conversion rate is 20% and you can assume a 50/50 split between control and treatment.

**How many samples will you need per group?**

```python
from statsmodels.stats.power import  zt_ind_solve_power
import statsmodels.stats.proportion as prop

std_effect = prop.proportion_effectsize(.20, .25)
zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=.05, power=.95)
```{{1}}

```
1807.76215
```{{2}}


`@script`
Furthermore, we can play around with the input values for the same example, and we get a different result. For instance, if we raise the power to .95, we require nearly 800 more observations since power and sample size are inversely related.


---
## Let's tackle this question in Python!

```yaml
type: "FinalSlide"
key: "87db49a5e8"
```

`@script`
We've gone over calculating sample size, touched on a couple complementary concepts, and looked at some examples, now let's put this into practice!

