---
title: Insert title here
key: f778970311895142fb07969270f68bfb

---
## Q9: How do you calculate needed sample size?

```yaml
type: "TitleSlide"
key: "c8e4861b47"
```

`@lower_third`

name: Conor Dewey
title: Data Scientist


`@script`
In the last lesson, we addressed analyzing results of an A/B test and deciding whether or not to ship the experiment. 

Now we'll take things a step further by looking at another popular interview question: How do you calculate needed sample size for an experiment?

Interviewers love this question since it hits on a number of fundamental statistical concepts that come up quite often in practice, all wrapped up into one question.


---
## Setting the scene

```yaml
type: "FullSlide"
key: "09d1b0c7bf"
```

`@part1`
* In other words: how many samples are needed per treatment?
   * Advised that you decide on a sample size prior to the experiment
   * No good general guidance when it comes to sample size; context dependent


`@script`
If you run an experiment, how do you decide how long it should run or in our terms: how many samples are needed per treatment?

It’s normally advised that you decide on a sample size in advance and wait until the experiment is over before ‘peeking’ at results. 

Despite what you may read in many guides to A/B testing, there is no good general guidance — it depends, mainly, on the frequency with which the desired goal is attained.


---
## Moving parts

```yaml
type: "FullSlide"
key: "2a28ffdaae"
```

`@part1`
* **Sample size:** number of observations needed per treatment
* **Effect size:** minimum size of the effect that you want to detect
* **Significance level:** significance level at which the test will be conducted
* **Power:** probability of detecting a specified effect size


`@script`
At a high-level, there are four moving parts when it comes to calculating required sample size: sample size, minimum effect size, significance level, and power.

Specify any three of them and the fourth can be calculated. Most commonly, you would want to calculate sample size, so you must specify the other three.

The minimum size of the effect that you want to detect in a statistical test ex. a 20% improvement
To distinguish between a .350 hitter and a .200 hitter in baseball, not that many at-bats are needed. However, distinguishing between a .300 hitter and a .280 hitter takes many more at-bats.

The statistical significance level at which the test will be conducted. This drives our type I error, the rate of false positives. The lower our significance level drops, the more samples we need.

The probability of detecting a given effect size with a given sample size. The key thing is that if we see something interesting, we want to make sure that we have enough power to conclude with high probability the the interesting result is actually statistically significant. This drives our type II error, the rate of false negatives. The more power we want, the bigger sample we need.


---
## Calculating sample size

```yaml
type: "FullSlide"
key: "b94f8b2f28"
```

`@part1`
* `statsmodels.stats.power`
   * `zt_ind_solve_power():` solves for any one parameter of a two sample z-test
   * `tt_ind_solve_power():` solves for any one parameter of two sample t-test
* `statsmodels.stats.proportion`
   * `proportion_effectsize():` computes standard effect size for a test comparing two proportions


`@script`
Now that we understand all the components that go into calculating sample size, let's get our hands dirty in Python. 

We'll be using a couple functions within the `statsmodels` package. Notably, the `zt_ind_solve_power` and `tt_ind_solve_power` functions take in all but one of the aforementioned components and calculate the remaining parameter. 

One preliminary step must be taken, as the power functions above take in standardized minimum effect difference. To get this, we can use `proportion_effectsize()` by inputting our baseline conversion rate and our desired minimum conversion rate.


---
## Example

```yaml
type: "FullCodeSlide"
key: "e209e0c32d"
```

`@part1`
```python
from statsmodels.stats.power import  zt_ind_solve_power
import statsmodels.stats.proportion as prop

std_effect = prop.proportion_effectsize(.20, .25)
zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=.05, power=.80)
```

```
1091.8962
```


`@script`
Let's look at an example. Since both functions work essentially the same, we'll focus on a z-test scenario where we are looking at conversion rates of a website.

First we input our baseline conversion rate of 20% and then add on our minimum effect size of 5% to get 25% as the second proportion. Once we have this result, we can plug it into our power function along with an alpha significance value of .05 and the common power value of 0.80. Once we run this function, we get a desired sample of around 1091 impressions.


---
## Example

```yaml
type: "FullCodeSlide"
key: "a7eb14f7cb"
disable_transition: true
```

`@part1`
```python
from statsmodels.stats.power import  zt_ind_solve_power
import statsmodels.stats.proportion as prop

std_effect = prop.proportion_effectsize(.20, .25)
zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=.05, power=.95)
```

```
1807.76215
```


`@script`
Furthermore, we can play around with the input values and we get a different result. For instance, if we raise the power to .95, we require nearly 800 more observations since power and sample size are inversely related.


---
## Let's answer this question in Python!

```yaml
type: "FinalSlide"
key: "87db49a5e8"
```

`@script`
We've gone over the concepts and looked at a couple examples, now let's put this into practice!

